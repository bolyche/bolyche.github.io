---
layout: post
date: 2023-08-08
title: "Prompt engineering"
categories: [ LLMs ]
tags: 
description: "Cursed python"
featured: false
hidden: false
image: assets/images/prompt-engineering-2.png
---

Today's mini-shitpost is not about code but just a wave-hands-in-the-air-bafflement at how apparently, prompt engineering has become not only a _thing_ but.. a job post?

I sure hope I'm wrong about the job part. I get emails from blogs sometimes and it certainly _sounded_ like a thing. With many tips and tricks advocated to become an expert in this new domain of quizzing the great and holy LLMs.

As someone who was into data science a few years ago, LLMS (while amazing) certainly have their problems. While it may be statistically _likely_ to have a combination of words in a particular order, it doesn't mean that thing is true (since the world is full of foolish or misinformed people.. and those people often produce content! Hey hey now don't look at me...), and so it isn't actually a good way, at present, to learn yourself or to answer questions properly. Or to _learn_ to answer questions properly. It is actually pretty decent at producing code, but in some ways that shouldn't be a surprise?

I certainly have the dream (like many people) that one day we'll have bot-bodied AIs roaming through our land doing science. Validating, interviewing, sifting evidence. AIs which deal with logic and knowledge. Maybe that's not even far away. Maybe it'll be fantastic.

But from my experience we're not there yet.

I was surprised but in many of my chats using ChatGPT-3.5 I kept encountering hallucinations. 

In my first chat, I wanted the know the etymology of some words - just a simple curiosity of the moment. The output immediately sounded amazing and I was massively impressed. But also I was raised to be a scientist and so I went fact checking. Immediately, I found out the answers were wrong. It was bizarre! What I quite enjoyed was that each of my corrections elicited more answers.. which from extended research also seemed wrong. Then if I asked for the origin.. it would backtrack. Example below

![LLMs](/assets/images/prompt-engineering-1.png)

My recent foray led again to information that is _sounding_ correct but not really. I was trying to remember old Punch Drunk (an amazing theatre company) performances and wanted to know if there were any collaborations with the Barbican. 

![LLMs](/assets/images/prompt-engineering-2.png)

But alas, as seen on the [Wikipedia page](https://en.wikipedia.org/wiki/The_Drowned_Man) and other places.. The Drownded Man was performed with the Royal National Theatre. & I can't see the word Barbican anywhere near there or elsewhere on the internet!

Maybe it's all about how you ask (or don't ask?) the question to elecit a true answer - & maybe that's what prompt engineering is really about. I will keep hold of my doubts though.